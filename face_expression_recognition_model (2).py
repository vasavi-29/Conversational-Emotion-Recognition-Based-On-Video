# -*- coding: utf-8 -*-
"""Face_expression_recognition_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12StemXh1VabySTzjrIBIGJjldVYGor1J
"""

from google.colab import drive
drive.mount('/content/drive')

from zipfile import ZipFile
# from PIL import Image
path = "/content/drive/MyDrive/face_dataset/Face_recognition_dataset.zip"

with ZipFile(path) as f:
  f.extractall("./data")
  print("Data in zip file extracted")

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
from PIL import Image

#img = Image.open("/content/data/archive(2)/images/train/angry/0.jpg")
img = Image.open("/content/data/images/train/angry/0.jpg")
img.size

data = tf.keras.utils.image_dataset_from_directory(
    "/content/data/images/train",
    image_size = (48,48),
    color_mode="grayscale",
    batch_size = 26417
)

model = tf.keras.Sequential([
    layers.Conv2D(18,5,padding="same",input_shape=(48,48,1)),
    layers.MaxPool2D(),
    layers.Conv2D(18,3,padding="same",activation="leaky_relu"),
    layers.MaxPool2D(),
    layers.Conv2D(36,3,padding="same",activation= "leaky_relu"),
    layers.MaxPool2D(),
    layers.Flatten(),
    layers.Dense(100,activation = "leaky_relu"),
    layers.Dense(10),
    layers.Activation("softmax")
])

model.summary()

model.compile(
    loss= tf.keras.losses.categorical_crossentropy,
    optimizer = tf.keras.optimizers.Adam(),
    metrics = ["Accuracy"]
)

for features,labels in data:
    features = tf.divide(features,255)
    labels = tf.one_hot(labels,10)
    record = model.fit(features,labels,batch_size = 27,epochs=100)

model.save("/content/drive/MyDrive/Colab Notebooks/datasets/Mini_project_face_expression_model.h5")

def detect_emotion(frame):
    # Preprocess the frame as needed
    preprocessed_frame = preprocess_frame(frame)

    # Perform emotion prediction using the loaded model
    emotion_label = model.predict(preprocessed_frame)

    return emotion_label

def preprocess_frame(frame):
    # Implement any necessary preprocessing steps on the frame
    # (e.g., resizing, normalization, format conversion, etc.)
    processed_frame = cv2.resize(frame, (48, 48))
    processed_frame = processed_frame / 255.0  # Normalize pixel values

    # Return the preprocessed frame
    return processed_frame

